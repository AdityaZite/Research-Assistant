{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:08:51.756538Z","iopub.execute_input":"2025-05-16T14:08:51.759583Z","iopub.status.idle":"2025-05-16T14:09:06.329843Z","shell.execute_reply.started":"2025-05-16T14:08:51.759497Z","shell.execute_reply":"2025-05-16T14:09:06.328569Z"}},"outputs":[{"name":"stderr","text":"2025-05-16 14:09:00.557027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747404540.619371     110 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747404540.636673     110 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"model_name = \"vennify/t5-base-grammar-correction\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:09:06.332088Z","iopub.execute_input":"2025-05-16T14:09:06.333075Z","iopub.status.idle":"2025-05-16T14:09:10.232764Z","shell.execute_reply.started":"2025-05-16T14:09:06.333030Z","shell.execute_reply":"2025-05-16T14:09:10.231602Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def correct_grammer(sentence, prefix=\"Correct the grammer \"):\n    input_text = f\"{prefix}: {sentence}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n    outputs = model.generate(**inputs, max_length=100)\n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if decoded.lower().startswith(prefix.lower()):\n        decoded = decoded[len(prefix):].lstrip(\": \").strip()\n    return decoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:19:45.394713Z","iopub.execute_input":"2025-05-16T14:19:45.395664Z","iopub.status.idle":"2025-05-16T14:19:45.403979Z","shell.execute_reply.started":"2025-05-16T14:19:45.395624Z","shell.execute_reply":"2025-05-16T14:19:45.402687Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import nltk\nimport random\nimport spacy\nfrom nltk.corpus import wordnet as wn\n\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:09:10.242002Z","iopub.execute_input":"2025-05-16T14:09:10.242306Z","iopub.status.idle":"2025-05-16T14:09:12.567309Z","shell.execute_reply.started":"2025-05-16T14:09:10.242280Z","shell.execute_reply":"2025-05-16T14:09:12.566124Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_synonym(word, pos_tag):\n    \"\"\"Return a synonym of the word using WordNet, or the original word if none found.\"\"\"\n    pos_map = {\n        'NOUN': wn.NOUN,\n        'VERB': wn.VERB,\n        'ADJ': wn.ADJ,\n        'ADV': wn.ADV\n    }\n    wn_pos = pos_map.get(pos_tag, None)\n    if wn_pos is None:\n        return word\n\n    synonyms = wn.synsets(word, pos=wn_pos)\n    lemmas = set()\n    for syn in synonyms:\n        for lemma in syn.lemmas():\n            if lemma.name().lower() != word.lower():\n                lemmas.add(lemma.name().replace(\"_\", \" \"))\n    if lemmas:\n        return random.choice(list(lemmas))\n    return word\n\ndef synonym_paraphraser(sentence, replacement_rate=0.3):\n    \"\"\"Replace some words in the sentence with synonyms based on replacement_rate.\"\"\"\n    doc = nlp(sentence)\n    new_tokens = []\n\n    for token in doc:\n        if (\n            token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV'] and \n            not token.is_stop and \n            token.is_alpha and \n            random.random() < replacement_rate\n        ):\n            synonym = get_synonym(token.text, token.pos_)\n            new_tokens.append(synonym)\n        else:\n            new_tokens.append(token.text)\n\n    return \" \".join(new_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:09:12.570716Z","iopub.execute_input":"2025-05-16T14:09:12.571462Z","iopub.status.idle":"2025-05-16T14:09:12.581485Z","shell.execute_reply.started":"2025-05-16T14:09:12.571415Z","shell.execute_reply":"2025-05-16T14:09:12.580397Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:14:00.059060Z","iopub.execute_input":"2025-05-16T14:14:00.060097Z","iopub.status.idle":"2025-05-16T14:14:00.071257Z","shell.execute_reply.started":"2025-05-16T14:14:00.060047Z","shell.execute_reply":"2025-05-16T14:14:00.068920Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def split_text(text):\n    return sent_tokenize(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:14:36.057889Z","iopub.execute_input":"2025-05-16T14:14:36.058298Z","iopub.status.idle":"2025-05-16T14:14:36.064005Z","shell.execute_reply.started":"2025-05-16T14:14:36.058269Z","shell.execute_reply":"2025-05-16T14:14:36.062763Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def convert_text(text):\n    sentences = split_text(text)\n    print(len(sentences))\n    ans = \"\"\n    for sentence in sentences:\n        sentence = synonym_paraphraser(sentence, replacement_rate=0.3)\n        sentence = correct_grammer(sentence)\n        ans += sentence.strip()\n        if not sentence.strip().endswith((\".\", \"!\", \"?\")):\n            ans += \". \"\n        else:\n            ans += \" \"\n    return ans.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:20:04.144468Z","iopub.execute_input":"2025-05-16T14:20:04.145183Z","iopub.status.idle":"2025-05-16T14:20:04.154739Z","shell.execute_reply.started":"2025-05-16T14:20:04.145017Z","shell.execute_reply":"2025-05-16T14:20:04.152974Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"text = \"\"\"The methodology for kidney disease detection using deep learning with the VGG16 model is as follows:\n\nData Collection\nA dataset of kidney images was used, comprising of two classes: normal kidneys and those affected by tumors. The images were collected from Kaggle image datasets, which had sufficient representation of both classes to create a complete dataset.\n\nData Preprocessing\nFollowing steps of preprocessing were used:\n\nImage Resizing: All images were resized to 224x224 pixels to match the input size required by the VGG16 architecture.\n\nNormalization: The pixel values of the images were normalized to a range of 0 to 1 by dividing by 255. This step makes the execution of the model faster as well as accurate.\n\nModel Selection\nThe VGG16 convolutional neural network (CNN) was selected for the classification of images. VGG16 is a deep learning model which was trained on the Kaggle dataset, which had thousands of kidney CT scan images. To level up the pre-trained weights, the VGG16 model was connected with the top fully connected layers removed. This improved the model’s capability.\n\nModel Architecture\nThe architecture of the model is as follows:\n\nPrimary Model: The convolutional layers of VGG16 model were used as the base for feature extraction. These layers were frozen in order to prevent any loss in pre-trained weights.\n\nMean Pooling: A mean pooling layer was used after the convolutional layer to reduce shape of the maps.\n\nFully Connected Layer: A series of fully connected layers were applied to the model, with the final layer using a softmax activation function for binary classification between normal and tumor classes.\n\nModel Training\nThe VGG16 model was trained with a softmax activation function, that had a learning rate of 0.001 to guide the training process. The dataset was divided into two sections, with 80% used for training and the remaining 20% reserved for testing. Training was carried out over 40 epochs to enhance the model’s performance and efficiency.\n\nEvaluation\nThe model’s performance was assessed using the validation dataset. To evaluate its classification capability, standard metrics including accuracy, precision, and loss were employed.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:28:14.516147Z","iopub.execute_input":"2025-05-16T14:28:14.516601Z","iopub.status.idle":"2025-05-16T14:28:14.523170Z","shell.execute_reply.started":"2025-05-16T14:28:14.516571Z","shell.execute_reply":"2025-05-16T14:28:14.521805Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"print(convert_text(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T14:28:15.423068Z","iopub.execute_input":"2025-05-16T14:28:15.423943Z","iopub.status.idle":"2025-05-16T14:28:43.566648Z","shell.execute_reply.started":"2025-05-16T14:28:15.423909Z","shell.execute_reply":"2025-05-16T14:28:43.565380Z"}},"outputs":[{"name":"stdout","text":"The methodological analysis for kidney disease detection using deep learning with the VGG16 example is as keep up : Data Collection A dataset of kidney images was used, comprising of two families : normal kidneys and those affected by tumors. The images were collected from Kaggle image datasets , which had sufficient representation of both classes to make an array dataset. Data Preprocessing Following steps of preprocessing were used : Image Resizing : All images were resized to 224x224 pixels to couple the input sizing required by the VGG16 computer architecture . Correct the grammar: standardisation: The pixel values of the images were normalized to a range of 0 to 1 by dividing by 255 . This footstep makes the writ of execution of the framework faster as well as accurate. Model Selection The VGG16 convolutional neural net ( CNN ) was selected for the compartmentalization of images. VGG16 is a thick erudition model which was trained on the Kaggle dataset , which had thousands of kidney CT scan images . To level up the pre-develop weights, the VGG16 poser was connected with the top fully connected layers removed. Correct the grammar : This improves the model’s capability. Model Architecture The architecture of the model is as succeed : Primary Model : The convolutional bed of VGG16 model were used as the base for feature extraction. These layers were frozen in order to prevent any loss in pre - trail system of weights. Correct the grammar : Mean Pooling : A mean pooling layer was used after the convolutional layer to reduce the shape of the maps. Correct the grammar : Fully Connected Layer : A series of fully link layers were applied to the model , with the final level using a softmax activation function for binary classification between normal and tumor classes . Model Training The VGG16 manikin was trained with a softmax activation function , that had a learning rate of 0.001 to guide the training physical process. The dataset was divided into two sections , with 80 % used for grooming and 20 % reserved for testing. Training was carried out over 40 epochs to enhance the model’s functioning and efficiency. Evaluation The model’s performance was assessed using the validation dataset. To evaluate its classification capability, standard metric including accuracy , precision , and expiration were employed.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}